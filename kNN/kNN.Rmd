---
title: "**k-Nearest Neighbors algorithm (k-NN)**"
author: Xavier Vivancos García
date: '`r Sys.Date()`'
output: 
  html_document:
    number_sections: yes
    theme: cosmo
    highlight: tango
---

# **El algoritmo k-NN**

<center>
_"Dime con quién andas y té diré quién eres"_
</center>

<br />

El algoritmo _k-Nearest Neighbors_ (_k_ vecinos más cercanos) es un método de clasificación **supervisada**, es decir, tenemos etiquetado nuestro conjunto de datos de entrenamiento. El algoritmo clasifica cada nueva instancia según el grupo al que pertenecen sus _k_ vecinos más cercanos (_k_ instancias del conjunto de entrenamiento más cercanas). Los pasos son los siguientes:

1. Normalizar los datos. Esta primera operación puede mejorar la exactitud del algoritmo.
2. Calcular la distancia entre la instancia a clasificar y cada una de las instancias del conjunto de entrenamiento. Se suele utilizar la distancia euclidiana, aunque también puede emplearse la distancia de Manhattan o Chebyshev, entre otras. 
3. Seleccionamos los _k_ elementos más cercanos
4. De los elementos seleccionados, ¿cuál es la clase/etiqueta mayoritaria? Una instancia es asignada a una clase si esta es la clase más frecuente entre los _k_ vecinos de entrenamiento.

Se trata de un algoritmo **basado en instancias** (_lazy learning_). Esto significa que el algoritmo no aprende un modelo o una generalización a partir del conjunto de entrenamiento, sino que compara cada nueva instancia con las instancias de entrenamiento. Sensible a _k_ y a la métrica de distancia o similitud utilizada. 

**Ventajas**: Sencillo de aprender e implementar.

**Inconvenientes**: Funciona bien con pocos datos, pero su rendimiento empeora para data sets con muchas filas y columnas. Recordemos que para cada nueva instancia a clasificar, el algoritmo tiene que calcular la distancia con todas las instancias de entrenamiento.

# **Ejemplo**

Dada la siguiente imagen...

<center>
![](C:/Users/xviva/OneDrive/Desktop/knn_ejemplo.png){width=35%}
</center>


- El punto verde es la instancia a clasificar
- Los triangulos y cuadrados son instancias del conjunto de entrenamiento, pertenecientes a dos grupos diferentes.

¿A qué grupo pertenecerá el punto verde? Depende del valor de _k_:

Para $k=1$ → Se clasificará como triángulo, ya que la instancia más cercana es un triángulo

Para $k=2$ → Las dos instancias más cercanas son triángulos, así que se clasificará como triángulo

Para $k=3$ → Las tres instancias más cercanas son dos triángulos y un cuadrado. Como predominan los triángulos, se clasificará como triángulo.

Para $k=5$ → Las cinco instancias más cercanas son dos triángulos y tres cuadrados, así que se clasificará como cuadrado.

... y así sucesivamente.

# **Código**

Vamos a utilizar el data set iris. Es una mierda, pero nos sirve perfectamente como ejemplo simple de cómo aplicar el algoritmo.

## Preparación de los datos

Primero leemos los datos, los normalizamos y los separamos en conjuntos de entrenamiento y test.

```{r}
# Cargamos librerías
library(class)

# Leemos datos 
data <- iris

# Normalizamos datos (excepto columna objetivo)
data[, -5] <- scale(iris[, -5])

# Resultados reproducibles
set.seed(1234)

# 70% entrenamiento y 30% test
ind <- sample(2, nrow(data), replace=TRUE, prob=c(0.7, 0.3))
trainData <- data[ind==1,]
testData <- data[ind==2,]
```

## Ejecución de k-NN

A continuación ejecutamos el algoritmo con los siguientes parámetros:

* `train`. Matriz o data frame con los datos de entrenamiento.

* `test`. Matriz o data frame con los datos de test. Estos son los datos que el algoritmo tiene que clasificar.

* `cl`. Etiquetas de los datos de entrenamiento.

* `k`. Número de vecinos.

```{r}
# Ejecución con k=1
KnnTestPrediction_k1 <- knn(trainData[,-5], testData[,-5],
                            trainData$Species, k=1, prob=TRUE)

# Ejecución con k=2
KnnTestPrediction_k2 <- knn(trainData[,-5], testData[,-5],
                            trainData$Species, k=2, prob=TRUE)

# Ejecución con k=3
KnnTestPrediction_k3 <- knn(trainData[,-5], testData[,-5],
                            trainData$Species, k=3, prob=TRUE)

# Ejecución con k=4
KnnTestPrediction_k4 <- knn(trainData[,-5], testData[,-5],
                            trainData$Species, k=4, prob=TRUE)
```

## Evaluación

Es momento de evaluar la precisión del algoritmo mediante matrices de confusión.

```{r}
# Matriz de confusión para k=1
table(testData$Species, KnnTestPrediction_k1)
```
¿Cómo interpretamos esta tabla?

* Las 10 observaciones pertenecientes a setosa han sido clasificadas correctamente como setosa

* Las 12 observaciones pertenecientes a versicolor han sido clasificadas correctamente como versicolor

* 2 observaciones pertenecientes a virginica han sido clasificadas erróneamente como versicolor.

El _accuracy_ es el siguiente:

```{r}
# Accuracy para k=1
sum(KnnTestPrediction_k1==testData$Species)/length(testData$Species)*100
```

Para los otros valores de _k_:

```{r}
# Matriz de confusión para k=2
table(testData$Species, KnnTestPrediction_k2)

# Accuracy para k=2
sum(KnnTestPrediction_k2==testData$Species)/length(testData$Species)*100

# Matriz de confusión para k=3
table(testData$Species, KnnTestPrediction_k3)

# Accuracy para k=3
sum(KnnTestPrediction_k3==testData$Species)/length(testData$Species)*100

# Matriz de confusión para k=4
table(testData$Species, KnnTestPrediction_k4)

# Accuracy para k=4
sum(KnnTestPrediction_k4==testData$Species)/length(testData$Species)*100
```

## Elección de _k_

¿Cuál es el valor óptimo de _k_?

```{r fig.align='center'}
# Empty variables
KnnTestPrediction <- list()
accuracy <- numeric()

# From k=1 to k=100...
for(k in 1:100){

  # KnnTestPrediction for each k
  KnnTestPrediction[[k]] <- knn(trainData[,-5], testData[,-5], trainData$Species, k, prob=TRUE)
    
  # Accuracy for each k   
  accuracy[k] <- sum(KnnTestPrediction[[k]]==testData$Species)/length(testData$Species)*100

}

# Accuracy vs Choice of k
plot(accuracy, type="b", col="dodgerblue", cex=1, pch=20,
     xlab="k, number of neighbors", ylab="% Classification accuracy", 
     main="Accuracy vs Neighbors")

# Add lines indicating k with best accuracy
abline(v=which(accuracy==max(accuracy)), col="darkorange", lwd=1.5)

# Add line for max accuracy seen
abline(h=max(accuracy), col="grey", lty=2)

# Add line for min accuracy seen 
abline(h=min(accuracy), col="grey", lty=2)
```
