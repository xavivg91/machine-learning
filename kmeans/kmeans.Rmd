---
title: "**k-means**"
author: Xavier Vivancos García
date: '`r Sys.Date()`'
output: 
  html_document:
    number_sections: yes
    theme: cosmo
    highlight: tango
---

# Preparación 

Leemos y normalizamos los datos.

```{r message=FALSE, warning=FALSE}
# Load libraries
library(tidyverse)
library(factoextra)
library(GGally)
library(gridExtra)

# Read data
data <- read.csv("https://raw.githubusercontent.com/xavivg91/machine-learning/main/kmeans/wine.csv")

# Normalization
dataNorm <- as.data.frame(scale(data))
```

## Ejecución 

```{r}
# Reproducible results
set.seed(800)

# Execution of k-means with k=2
data_k2 <- kmeans(dataNorm, centers=2)
```

El _output_ nos proporciona la siguiente información acerca de la partición:

* `cluster`. Vector que nos indica el clúster asociado a cada observación. 

* `centers`. Posiciones de los centroides.

* `size`. Número de observaciones en cada clúster.

```{r}
# Cluster to which each point is allocated
data_k2$cluster

# Cluster centers
data_k2$centers

# Cluster size
data_k2$size
```

Adicionalmente, también podemos consultar las siguientes métricas:

* `betweenss`. Varianza entre grupos. Suma de las distancias entre centroides. En una segmentación óptima, este valor debe ser lo más grande posible, ya que nos interesa obtener clústers heterogéneos.
 
* `withinss`. Varianza dentro de grupos. Suma de las distancias entre cada punto del clúster y su centroide. En una segmentación óptima, este valor debe ser lo más pequeño posible, puesto que nos interesa que las observaciones dentro de un mismo grupo sean similares. `withinss`nos devuelve una varianza por cada clúster.

* `tot.withinss`. Suma de las varianzas dentro de grupos. 

* `totss`. Suma de `betweenss` y `withinss`.

```{r}
# Between-cluster sum of squares
data_k2$betweenss

# Within-cluster sum of squares
data_k2$withinss

# Total within-cluster sum of squares 
data_k2$tot.withinss

# Total sum of squares
data_k2$totss
```
## ¿Cuántos clústers?

El siguiente gráfico nos representa la varianza entre grupos y dentro de grupos en función del número de clústers escogidos.

```{r fig.align='center'}
bss <- numeric()
wss <- numeric()

# Reproducible results
set.seed(800)

# Run the algorithm for different values of k
for(i in 1:10){

  # For each k, calculate betweenss and tot.withinss
  bss[i] <- kmeans(dataNorm, centers=i)$betweenss
  wss[i] <- kmeans(dataNorm, centers=i)$tot.withinss

}

df <- data.frame(k=1:10, bss, wss)
df <- df %>% 
  gather(Varianzas, Valores, 2:3) %>%
  mutate(Varianzas=fct_recode(Varianzas, "Varianza dentro de grupos"="wss",
                              "Varianza entre grupos"="bss"))

# Visualization
df %>%
  ggplot(aes(x=k, y=Valores, colour=Varianzas)) +
  geom_line() +
  geom_point(size=2) +
  labs(title="Varianza entre grupos y dentro de grupos vs Elección de k", 
       y="Varianza", x="Número de clusters", subtitle="Método del codo") +
  scale_x_continuous(breaks=1:10) +
  geom_vline(xintercept=3, linetype="dotted") +
  theme_bw() +
  theme(legend.position="bottom",
        legend.title=element_blank())

```

Para seleccionar el valor óptimo de _k_, se escoge el punto a partir del cual se dejan de producir variaciones significativas de los valores de varianza entre grupos y dentro de grupos (método del codo). En este caso, queda claro que a partir de tres clústers dejan de producirse diferencias significativas.

```{r message=FALSE, warning=FALSE, fig.align='center'}
# Reproducible results
set.seed(800)

# Single function to compute the “Elbow method”
fviz_nbclust(data, kmeans, method="wss")
```


## Resultados

Calculamos los valores medios de cada clúster.

```{r}
# Reproducible results
set.seed(800)

# Execution of k-means with k=3
data_k3 <- kmeans(dataNorm, centers=3)

# Mean values of each cluster
aggregate(data, by=list(data_k3$cluster), mean)
```

## Visualizaciones

Visualización hecha con la librería GGally.

```{r message=FALSE, warning=FALSE, fig.align='center'}
# Clustering 
ggpairs(cbind(data, Cluster=as.factor(data_k3$cluster)),
        columns=1:6, aes(colour=Cluster, alpha=0.5),
        lower=list(continuous="points"),
        upper=list(continuous="blank"),
        axisLabels="none", switch="both") +
        theme_bw()
```
<br />

En los siguientes gráficos puede verse muy claramente como tres grupos son los ideales para llevar a cabo la segmentación. Visualizaciones hechas con la librería factoextra.

```{r fig.align='center', message=FALSE, warning=FALSE}
# Reproducible results
set.seed(800)

# Execution of k-means
kmeans2 <- kmeans(dataNorm, centers=2)
kmeans3 <- kmeans(dataNorm, centers=3)
kmeans4 <- kmeans(dataNorm, centers=4)
kmeans5 <- kmeans(dataNorm, centers=5)

# Visualization
p1 <- fviz_cluster(kmeans2, geom="point", data=dataNorm) + ggtitle("K=2") + theme_bw()
p2 <- fviz_cluster(kmeans3, geom="point", data=dataNorm) + ggtitle("K=3") + theme_bw()
p3 <- fviz_cluster(kmeans4, geom="point", data=dataNorm) + ggtitle("K=4") + theme_bw()
p4 <- fviz_cluster(kmeans5, geom="point", data=dataNorm) + ggtitle("K=5") + theme_bw()

# Subplot
grid.arrange(p1, p2, p3, p4, nrow=2)
```
<br />

Si quisiéramos la visualización con etiquetas...

```{r fig.align='center'}
# Dummy example 

# Labels 
rownames(dataNorm) <- 1:nrow(data)
  
# Visualization with labels
fviz_cluster(kmeans3, data=dataNorm, star.plot=TRUE, repel=TRUE) + 
  ggtitle("K=4") + 
  theme_bw() +
  theme(legend.position="none")
```
<br />
 
A continuación, un gráfico con los datos normalizados agrupados por clústers, con sus respectivos centroides (puntos negros).

```{r fig.align='center'}
# Cluster to which each point is allocated
dataNorm$cluster <- kmeans3$cluster

# Visualization
ggplot() + 
  
  # Normalized data
  geom_point(aes(x=Alcohol, y=Ash, color=cluster), data=dataNorm, size=2, show.legend=FALSE) +
  
  # Centroids
  geom_point(aes(x=kmeans3$centers[, 1], y=kmeans3$centers[, 3]), color='black', size=5, shape=17) +
  
  labs(title="k-means con k=3", subtitle="Datos normalizados") +
  scale_colour_gradientn(colours=rainbow(4)) +
  theme_bw()
```
<br />

Con los datos originales...

```{r fig.align='center'}
# Cluster to which each point is allocated
data$cluster <- kmeans3$cluster

# Mean values of each cluster
centroids <- aggregate(data, by=list(data_k3$cluster), mean)

# Visualization
ggplot() + 
  
  # Original data
  geom_point(aes(x=Alcohol, y=Malic_Acid, color=cluster), data=data, size=2, show.legend=FALSE) +
  
  # Centroids
  geom_point(aes(x=centroids[, 2], y=centroids[, 3]), color='black', size=5, shape=17) +
  
  labs(title="k-means con k=3", subtitle="Datos originales") +
  scale_colour_gradientn(colours=rainbow(4)) +
  theme_bw()
```
<br />

Podemos ver las diferencias entre estos dos últimos gráficos en los valores de los ejes.