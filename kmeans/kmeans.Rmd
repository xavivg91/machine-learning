---
title: "**k-means**"
author: Xavier Vivancos García
date: '`r Sys.Date()`'
output: 
  html_document:
    number_sections: yes
    theme: cosmo
    highlight: tango
---

# **El algoritmo k-means**

_k-means_ es un algoritmo **no supervisado** que se utiliza para **agrupar** observaciones que comparten características similares (clústers). Partición de un conjunto de _n_ observaciones en _k_ grupos o clústers. ¿No supervisado? Sí, significa que las observaciones dadas en el conjunto de datos no están etiquetadas, no hay ningún resultado que predecir. El algoritmo sigue los siguientes pasos:

1. **Inicialización**. Se establecen _k_ centroides en el espacio de los datos, escogidos aleatoriamente. El valor _k_ lo indica el usuario al ejecutar el algoritmo. 

  + Método de Forgy. Elige aleatoriamente _k_ observaciones del conjunto de datos y las            utiliza como centroides. Tiende a dispersar los centroides iniciales.
  + Método de partición aleatoria. Asigna aleatoriamente un clúster para cada observación y        después procede a la etapa de actualización. Ubica los centroides cerca del centro del         conjunto de datos.

2. **Asignación**. Cada observación de los datos es asignada a su centroide más cercano, utilizando la distancia euclidiana.

3. **Actualización**. Se actualiza la posición del centroide de cada grupo, tomando como nuevo centroide la posición del promedio de las observaciones pertenecientes.

4.- Repetir los pasos 2 y 3 hasta que los centroides no se mueven, o se mueven por debajo de una distancia umbral.

¿Cuál es el objetivo? Una segmentación óptima es aquella donde las observaciones pertenecientes a un mismo grupo son lo más homogéneas posible y las observaciones pertenecientes a distintos grupos son lo más heterogéneas posible. La varianza dentro de grupos debe ser reducida (observaciones dentro de un mismo grupo tienen que ser similares) y la varianza entre grupos debe ser grande (observaciones de distintos grupos tienen que ser distintas). 

Por tanto, el algoritmo _k-means_ resuelve un problema de optimización, siendo la función a optimizar (minimizar) la suma de las distancias de cada objeto al centroide de su cluster. Otro criterio podría ser el de maximizar la suma de distancias entre grupos.

**Ventajas**: Simple, flexible y eficiente. Los resultados son fáciles de interpretar.

**Inconvenientes**: Definir número de grupos (_elbow method_). Resultado varía en función de la semilla escogida en la inicialización. Sensible a _outliers_.

# **Ejemplo**

<center>
![](C:/Users/xviva/OneDrive/Desktop/kmeans_ejemplo.png){width=75%}
</center>

<br />

1. En la primera imagen, _k-means_ genera aleatoriamente tres centroides en el espacio de los datos ($k=3$). No se ha empleado el método de Forgy, puesto que no ha utilizado observaciones de los datos como centroides iniciales.

2. En la segunda imagen, se asocia cada observación al centroide más cercando, formando tres clusters. Las particiones se representan mediante diagramas de Voronoi.

3. A continuación, se actualiza la posición del centroide de cada grupo, en función del promedio de las observaciones pertenecientes. En el grupo rojo sólo hay una observación, por eso el centroide se coloca justo encima de ella.

4. Se repiten los pasos 2 y 3 hasta que se logre la convergencia.

# **Código**

## Preparación de los datos

Leemos y normalizamos los datos.

```{r message=FALSE, warning=FALSE}
# Load libraries
library(tidyverse)
library(factoextra)
library(GGally)
library(gridExtra)

# Load data
data <- read.csv("C:/Users/xviva/OneDrive/Desktop/wine.csv")

# Normalization
dataNorm <- as.data.frame(scale(data))
```

## Ejecución del algoritmo

```{r}
# Execution of k-means with k=2
set.seed(800)
data_k2 <- kmeans(dataNorm, centers=2)
```

El _output_ nos proporciona la siguiente información acerca de la partición:

* `cluster.` Vector que nos indica el clúster asociado a cada observación. 

* `centers.` Posiciones de los centroides.

* `size.` Número de observaciones en cada clúster.

```{r}
# Cluster to which each point is allocated
data_k2$cluster

# Cluster centers
data_k2$centers

# Cluster size
data_k2$size
```

Además, también podemos consultar las siguientes métricas:

* `betweenss`. Varianza entre grupos. En una segmentación óptima, este valor debe ser lo más grande posible, puesto que las observaciones de distintos grupos tienen que ser distintas.
 
* `withinss`. Varianza dentro de grupos. En una segmentación óptima, este valor debe ser lo más pequeño posible, puesto que las observaciones dentro de un mismo grupo tienen que ser similares. Un valor por cada clúster.

* `tot.withinss`. Total de la varianza dentro de grupos. 

* `totss`. Suma total de varianza entre y dentro de grupos.

```{r}
# Between-cluster sum of squares
data_k2$betweenss

# Within-cluster sum of squares
data_k2$withinss

# Total within-cluster sum of squares 
data_k2$tot.withinss

# Total sum of squares
data_k2$totss
```
## ¿Cuántos clústers?

A continuación definimos el modo de obtener un gráfico que nos represente la varianza entre grupos y dentro de grupos en función del número de grupos.

```{r fig.align='center'}
bss <- numeric()
wss <- numeric()

# Semilla
set.seed(800)

# Ejecutamos el algoritmo k-means para diferentes valores de k
for(i in 1:10){

  # Para cada k, calculamos betweenss y tot.withinss
  bss[i] <- kmeans(dataNorm, centers=i)$betweenss
  wss[i] <- kmeans(dataNorm, centers=i)$tot.withinss

}

# Creación de data frame
df <- data.frame(k=1:10, bss, wss)
df <- df %>% 
  gather(Varianzas, Valores, 2:3) %>%
  mutate(Varianzas=fct_recode(Varianzas, "Varianza dentro de grupos"="wss",
                              "Varianza entre grupos"="bss"))

# Varianza entre grupos y dentro de grupos vs Elección de k
df %>%
  ggplot(aes(x=k, y=Valores, colour=Varianzas)) +
  geom_line() +
  geom_point(size=2) +
  labs(title="Varianza entre grupos y dentro de grupos vs Elección de k", 
       y="Varianza", x="Número de clusters", subtitle="Método del codo") +
  scale_x_continuous(breaks=1:10) +
  geom_vline(xintercept=3, linetype="dotted") +
  theme_bw() +
  theme(legend.position="bottom",
        legend.title=element_blank())

```
Para seleccionar el valor óptimo de k, se escoge ese punto en donde se dejan de producir variaciones significativas de los valores de varianza entre grupos y dentro de grupos (método del codo). En este caso, queda claro que a partir de tres clústers dejan de producirse diferencias significativas.

## Resultados

En las siguientes líneas calculamos los valores medios de cada clúster.

```{r}
# Execution of k-means with k=3
set.seed(800)

data_k3 <- kmeans(dataNorm, centers=3)

# Mean values of each cluster
aggregate(data, by=list(data_k3$cluster), mean)
```


```{r message=FALSE, warning=FALSE, fig.align='center'}
# Clustering 
ggpairs(cbind(data, Cluster=as.factor(data_k3$cluster)),
        columns=1:6, aes(colour=Cluster, alpha=0.5),
        lower=list(continuous="points"),
        upper=list(continuous="blank"),
        axisLabels="none", switch="both") +
        theme_bw()
```
<br />

En los siguientes gráficos puede verse muy claramente como tres grupos son los ideales para llevar a cabo la segmentación.

```{r fig.align='center'}
# Semilla
set.seed(800)

# Ejecución del algoritmo k-means
kmeans2 <- kmeans(dataNorm, centers=2)
kmeans3 <- kmeans(dataNorm, centers=3)
kmeans4 <- kmeans(dataNorm, centers=4)
kmeans5 <- kmeans(dataNorm, centers=5)


# Visualización
p1 <- fviz_cluster(kmeans2, geom="point", data=dataNorm) + ggtitle("K=2") + theme_bw()
p2 <- fviz_cluster(kmeans3, geom="point", data=dataNorm) + ggtitle("K=3") + theme_bw()
p3 <- fviz_cluster(kmeans4, geom="point", data=dataNorm) + ggtitle("K=4") + theme_bw()
p4 <- fviz_cluster(kmeans5, geom="point", data=dataNorm) + ggtitle("K=5") + theme_bw()

# Subplot
grid.arrange(p1, p2, p3, p4, nrow=2)
```
<br />

Si quisiéramos con labels...

```{r fig.align='center'}
# Labels como rownames
rownames(dataNorm) <- 1:nrow(data)
  
# Clusters con los labels
fviz_cluster(kmeans3, data=dataNorm, star.plot=TRUE, repel=TRUE) + 
  ggtitle("K=4") + 
  theme_bw() +
  theme(legend.position="none")
```
<br />
 
A continuación un gráfico con los datos normalizados agrupados por clústers, con sus respectivo centroides (puntos negros).

```{r fig.align='center'}
dataNorm$cluster <- kmeans3$cluster

ggplot() + 
  geom_point(aes(x=Alcohol, y=Ash, color=cluster), data=dataNorm, size=2, show.legend=FALSE) +
  scale_colour_gradientn(colours=rainbow(4)) +
  geom_point(aes(x=kmeans3$centers[, 1], y=kmeans3$centers[, 3]), color='black', size=5) +
  theme_bw()
```
<br />

Con los datos originales...

```{r fig.align='center'}
data$cluster <- kmeans3$cluster

# Mean values of each cluster
mean <- aggregate(data, by=list(data_k3$cluster), mean)

ggplot() + 
  geom_point(aes(x=Alcohol, y=Malic_Acid, color=cluster), data=data, size=2, show.legend=FALSE) +
  scale_colour_gradientn(colours=rainbow(4)) +
  geom_point(aes(x=mean[, 2], y=mean[, 3]), color='black', size=5) +
  theme_bw()
```
